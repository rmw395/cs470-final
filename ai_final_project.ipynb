{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxgBX0YXu1du"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXyRfd35yRPd"
      },
      "source": [
        "# Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jRvHTlW0DYA"
      },
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUzUupva0Fxw"
      },
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAssSW_I0GvA"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as skl #.model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "e0MQ0eo0MnmB"
      },
      "outputs": [],
      "source": [
        "def performance(model, data):\n",
        "    # run with specified model\n",
        "    # determine which training dataset performed the best\n",
        "    # print the result\n",
        "\n",
        "    # wait no... ugh WHAT\n",
        "    \n",
        "    print(\"result:\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData():\n",
        "    # load data\n",
        "    spam_data = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "    # shuffle data\n",
        "    spam_data = spam_data.sample(frac=1)\n",
        "\n",
        "    # first 80% is training, last 20% is test\n",
        "    training = spam_data[:int(len(spam_data)*0.8)]\n",
        "    test = spam_data[int(len(spam_data)*0.8):]\n",
        "\n",
        "    # use cross validation on training ?\n",
        "\n",
        "    # get kfold\n",
        "    kfold = skl.model_selection.KFold(5)\n",
        "\n",
        "    # loop over split values\n",
        "    for train_index, valid_index in kfold.split(training):\n",
        "        # get validation and training datasets\n",
        "        sub_validation = training.iloc[valid_index]\n",
        "        sub_training = training.iloc[train_index]\n",
        "\n",
        "    # TODO TEMP\n",
        "    return sub_validation, sub_training\n",
        "\n",
        "    # train on models\n",
        "\n",
        "    # check performance\n",
        "\n",
        "    '''\n",
        "    random.shuffle(data) # change if you are using pandas dataframe\n",
        "    training = data[:int(len(data)*0.8)]\n",
        "    test = data[int(len(data)*0.8):]\n",
        "\n",
        "    fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "    for train_idx, val_idx in fold5.split(training):\n",
        "        sub_val = training[val_idx]\n",
        "        sub_train = training[train_idx]\n",
        "        clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "        performance(clf, test) # test the model on test dataset\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "naive bayes\n",
        "\n",
        "calculate probability of spam for each attribute (word)\n",
        "- compare to each email in test\n",
        "- see if the prediction is right\n",
        "- compare that to test dataset (take the higher probablity)\n",
        "\n",
        "nvm. fr:\n",
        "\n",
        "for each email in test...?\n",
        "- use the probabilities calculated from the training dataset\n",
        "- multiply the probabilities together?? like if it has a word in it,\n",
        "- (use the probability (from training) that it is spam)\n",
        "- multiply THAT by the general probability that smth is spam\n",
        "\n",
        "then also do that for prob. not spam\n",
        "\n",
        "whichever probability is  higher (spam/not spam). say that it is that.\n",
        "\n",
        "keep track of well this training dataset did. (keep track of all of them actually.)  \n",
        "and then like. one of them did the best. yaaaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def naiveBayes(training, test):\n",
        "\n",
        "    pSpam = dict()\n",
        "    pNot = dict()\n",
        "\n",
        "    # data.iloc[:, [i]]\n",
        "    # where : selects all rows\n",
        "    # and i is the current column\n",
        "\n",
        "    # calculate probability of spam for each word\n",
        "        # use a dict?\n",
        "\n",
        "    #'''\n",
        "    # loop over all columns (except the last 5)\n",
        "    for col in range (54):\n",
        "        # reset counts\n",
        "        wordCount = 0\n",
        "        spamCount = 0\n",
        "        notCount = 0\n",
        "        # loop over all rows\n",
        "        for row in training.iterrows(): # how many rows ?\n",
        "            # if word is in the row\n",
        "            if training[row, col] > 0:\n",
        "                # update word count\n",
        "                wordCount = wordCount + 1\n",
        "                # if it's spam, add to spam count\n",
        "                if training[row, 57] == 1:\n",
        "                    spamCount = spamCount + 1\n",
        "                # else, add to not spam count\n",
        "                else:\n",
        "                    notCount = notCount + 1\n",
        "        # after looping over all rows\n",
        "        # do some division. or whatever\n",
        "        # add to dicts\n",
        "        pSpam[col] = spamCount / wordCount\n",
        "        pNot[col] = notCount / wordCount\n",
        "    # end dataset loop\n",
        "    #'''\n",
        "\n",
        "    # calculate overall probability that something is spam\n",
        "\n",
        "    totalSpam = 0\n",
        "    totalNot = 0\n",
        "\n",
        "    # loop over the rows, count how many are spam and how many aren't\n",
        "    # do some division or whatver\n",
        "    for row in training.iterrows():\n",
        "        if training[row, 57] == 1:\n",
        "            totalSpam = totalSpam + 1\n",
        "        else:\n",
        "            totalNot = totalNot + 1\n",
        "\n",
        "    prob = dict()\n",
        "\n",
        "    # for each email in test\n",
        "    for row in training.iterrows():\n",
        "        emailSpam = totalSpam\n",
        "        emailNot = totalNot\n",
        "\n",
        "        # for each word...\n",
        "        for col in range(54):\n",
        "            # if it has the word\n",
        "            if training[row, col] > 0:\n",
        "                # multiply probability that word = spam by probability of spam\n",
        "                emailSpam = emailSpam * pSpam[col]\n",
        "                # multiply probability that word != spam by probability of not spam\n",
        "                emailNot = emailNot * pNot[col]\n",
        "        # whichever probability is higher. say it is that.\n",
        "        if emailSpam > emailNot:\n",
        "            prob[col] = 1\n",
        "        else:\n",
        "            prob[col] = 0\n",
        "\n",
        "    return prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "knn\n",
        "\n",
        "pair-wise distance sklearn\n",
        "- will return a matrix with all the distances from everything to everything else\n",
        "\n",
        "go through all test emails\n",
        "- find closest traning emails to the current test email\n",
        "- pick the k closest neighbors, find the most frequent result\n",
        "- decide that it is that one.\n",
        "\n",
        "also in general we are just finding which training dataset performed the best and printing the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn(training, test):\n",
        "\n",
        "    # uh\n",
        "    #  sklearn.metrics.pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
        "    distances = skl.metrics.pairwise_distances(training, test)\n",
        "\n",
        "    pred = dict()\n",
        "\n",
        "    for row in range(len(distances)): # that probably doesn't work but yknow\n",
        "        # fill array with the first val\n",
        "        close = list()\n",
        "        closest = distances[row][0]\n",
        "        # find closest rows...\n",
        "        for col in range(len(distances[row])):\n",
        "            if distances[row][col] <= closest:\n",
        "                closest = distances[row][col]\n",
        "                close.append(col)\n",
        "        # just get the 5 closest\n",
        "        # i am assuming that there are at least 5 that are in the list but. there probably are.\n",
        "        close = close[-5:]\n",
        "\n",
        "        spam = 0\n",
        "        for i in range(5):\n",
        "            spam = spam + test[i, 57]\n",
        "        \n",
        "        if spam > 5 - spam:\n",
        "            pred[row] = 1\n",
        "        else:\n",
        "            pred[row] = 0\n",
        "    \n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "logistic regression\n",
        "\n",
        "tbh just use the example code and formula stuff lol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lr(data):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "final actual real things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "(0, 57)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: (0, 57)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m val, train \u001b[38;5;241m=\u001b[39m loadData()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#naiveBayes(train, val)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[95], line 24\u001b[0m, in \u001b[0;36mknn\u001b[1;34m(training, test)\u001b[0m\n\u001b[0;32m     22\u001b[0m spam \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     spam \u001b[38;5;241m=\u001b[39m spam \u001b[38;5;241m+\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m57\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spam \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m-\u001b[39m spam:\n\u001b[0;32m     27\u001b[0m     pred[row] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: (0, 57)"
          ]
        }
      ],
      "source": [
        "val, train = loadData()\n",
        "naiveBayes(train, val)\n",
        "knn(train, val)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
