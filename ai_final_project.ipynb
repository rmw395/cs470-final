{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxgBX0YXu1du"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXyRfd35yRPd"
      },
      "source": [
        "# Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jRvHTlW0DYA"
      },
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUzUupva0Fxw"
      },
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAssSW_I0GvA"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as skl #.model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "e0MQ0eo0MnmB"
      },
      "outputs": [],
      "source": [
        "def performance(model, data):\n",
        "    # run with specified model\n",
        "    # determine which training dataset performed the best\n",
        "    # print the result\n",
        "\n",
        "    # wait no... ugh WHAT\n",
        "    \n",
        "    print(\"result:\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData():\n",
        "    # load data\n",
        "    spam_data = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "    # shuffle data\n",
        "    spam_data = spam_data.sample(frac=1)\n",
        "\n",
        "    # first 80% is training, last 20% is test\n",
        "    training = spam_data[:int(len(spam_data)*0.8)]\n",
        "    test = spam_data[int(len(spam_data)*0.8):]\n",
        "\n",
        "    # use cross validation on training ?\n",
        "\n",
        "    # get kfold\n",
        "    kfold = skl.model_selection.KFold(5)\n",
        "\n",
        "    # loop over split values\n",
        "    for train_index, valid_index in kfold.split(training):\n",
        "        # get validation and training datasets\n",
        "        sub_validation = training.iloc[valid_index]\n",
        "        sub_training = training.iloc[train_index]\n",
        "\n",
        "    # TODO TEMP\n",
        "    return sub_validation, sub_training\n",
        "\n",
        "    # train on models\n",
        "\n",
        "    # check performance\n",
        "\n",
        "    '''\n",
        "    random.shuffle(data) # change if you are using pandas dataframe\n",
        "    training = data[:int(len(data)*0.8)]\n",
        "    test = data[int(len(data)*0.8):]\n",
        "\n",
        "    fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "    for train_idx, val_idx in fold5.split(training):\n",
        "        sub_val = training[val_idx]\n",
        "        sub_train = training[train_idx]\n",
        "        clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "        performance(clf, test) # test the model on test dataset\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "naive bayes\n",
        "\n",
        "calculate probability of spam for each attribute (word)\n",
        "- compare to each email in test\n",
        "- see if the prediction is right\n",
        "- compare that to test dataset (take the higher probablity)\n",
        "\n",
        "nvm. fr:\n",
        "\n",
        "for each email in test...?\n",
        "- use the probabilities calculated from the training dataset\n",
        "- multiply the probabilities together?? like if it has a word in it,\n",
        "- (use the probability (from training) that it is spam)\n",
        "- multiply THAT by the general probability that smth is spam\n",
        "\n",
        "then also do that for prob. not spam\n",
        "\n",
        "whichever probability is  higher (spam/not spam). say that it is that.\n",
        "\n",
        "keep track of well this training dataset did. (keep track of all of them actually.)  \n",
        "and then like. one of them did the best. yaaaa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def naiveBayes(training, test):\n",
        "\n",
        "    # wah?\n",
        "\n",
        "    # my model in this situation is my probabilities?\n",
        "    pSpam = dict()\n",
        "    pNot = dict()\n",
        "\n",
        "    # data.iloc[:, [i]]\n",
        "    # where : selects all rows\n",
        "    # and i is the current column\n",
        "\n",
        "    # calculate probability of spam for each word\n",
        "        # use a dict?\n",
        "\n",
        "    #'''\n",
        "    # loop over all columns (except the last 5)\n",
        "    for col in range (54):\n",
        "        # reset counts\n",
        "        wordCount = 0\n",
        "        spamCount = 0\n",
        "        notCount = 0\n",
        "        # loop over all rows\n",
        "        for row in training.iterrows(): # how many rows ?\n",
        "            # TODO remove\n",
        "            print(row, \"\", col)\n",
        "\n",
        "            # if word is in the row\n",
        "            if training[row, col] > 0:\n",
        "                # update word count\n",
        "                wordCount = wordCount + 1\n",
        "                # if it's spam, add to spam count\n",
        "                if training[row, 57] == 1:\n",
        "                    spamCount = spamCount + 1\n",
        "                # else, add to not spam count\n",
        "                else:\n",
        "                    notCount = notCount + 1\n",
        "        # after looping over all rows\n",
        "        # do some division. or whatever\n",
        "        # add to dicts\n",
        "        pSpam[col] = spamCount / wordCount\n",
        "        pNot[col] = notCount / wordCount\n",
        "    # end dataset loop\n",
        "    #'''\n",
        "\n",
        "    # calculate overall probability that something is spam\n",
        "\n",
        "    # loop over the rows, count how many are spam and how many aren't\n",
        "    # do some division or whatver\n",
        "\n",
        "    # for each email in test\n",
        "        # for each word...\n",
        "            # if it has the word\n",
        "                # multiply probability that word = spam by probability of spam\n",
        "                # multiply probability that word != spam by probability of not spam\n",
        "    \n",
        "    # whichever probability is higher. say it is that.\n",
        "\n",
        "    # check if we were correct ??\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "knn\n",
        "\n",
        "pair-wise distance sklearn\n",
        "- will return a matrix with all the distances from everything to everything else\n",
        "\n",
        "go through all test emails\n",
        "- find closest traning emails to the current test email\n",
        "- pick the k closest neighbors, find the most frequent result\n",
        "- decide that it is that one.\n",
        "\n",
        "also in general we are just finding which training dataset performed the best and printing the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn(training, test):\n",
        "\n",
        "    # uh\n",
        "    #  sklearn.metrics.pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
        "    distances = skl.metrics.pairwise_distances(training, test)\n",
        "    print(distances)\n",
        "\n",
        "    #for row in len(distances): # that probably doesn't work but yknow\n",
        "        # find closest rows...\n",
        "\n",
        "    \n",
        "    # what"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "logistic regression\n",
        "\n",
        "tbh just use the example code and formula stuff lol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lr(data):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "final actual real things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 470.52035396  579.38503064  474.69155428 ...  426.35385055\n",
            "   464.64751379  491.20675123]\n",
            " [  72.10814851   37.32740682   68.3908666  ...  116.14770738\n",
            "    78.26870665   51.62304851]\n",
            " [ 762.34989114  869.36851657  766.86509855 ...  718.2215339\n",
            "   756.85742865  781.49617041]\n",
            " ...\n",
            " [ 290.19511248  399.11899791  294.35768007 ...  246.09724695\n",
            "   284.31615963  311.03059288]\n",
            " [ 106.80994581   14.72481525  102.61674022 ...  151.11323308\n",
            "   112.63712587   87.5995245 ]\n",
            " [3371.57670666 3480.49823291 3375.66856895 ... 3327.44536167\n",
            "  3365.64156738 3392.33279951]]\n"
          ]
        }
      ],
      "source": [
        "val, train = loadData()\n",
        "#naiveBayes(train, val)\n",
        "knn(train, val)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
