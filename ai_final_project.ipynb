{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAssSW_I0GvA"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as skl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadData():\n",
        "    # load data\n",
        "    spam_data = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "    # shuffle data\n",
        "    spam_data = spam_data.sample(frac=1)\n",
        "\n",
        "    # first 80% is training, last 20% is test\n",
        "    training = spam_data[:int(len(spam_data)*0.8)]\n",
        "    test = spam_data[int(len(spam_data)*0.8):]\n",
        "\n",
        "    # get kfold\n",
        "    kfold = skl.model_selection.KFold(5)\n",
        "\n",
        "    # loop over split values\n",
        "    for train_index, valid_index in kfold.split(training):\n",
        "        # get validation and training datasets\n",
        "        sub_validation = training.iloc[valid_index]\n",
        "        sub_training = training.iloc[train_index]\n",
        "\n",
        "    return sub_validation, sub_training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def naiveBayes(training, test):\n",
        "\n",
        "    pSpam = dict()\n",
        "    pNot = dict()\n",
        "\n",
        "    # loop over all columns (except the last 5)\n",
        "    for col in range (54):\n",
        "        # reset counts\n",
        "        wordCount = 0\n",
        "        spamCount = 0\n",
        "        notCount = 0\n",
        "        # loop over all rows\n",
        "        for row in training.iterrows(): # how many rows ?\n",
        "            # if word is in the row\n",
        "            if training[row, col] > 0:\n",
        "                # update word count\n",
        "                wordCount = wordCount + 1\n",
        "                # if it's spam, add to spam count\n",
        "                if training[row, 57] == 1:\n",
        "                    spamCount = spamCount + 1\n",
        "                # else, add to not spam count\n",
        "                else:\n",
        "                    notCount = notCount + 1\n",
        "        # after looping over all rows\n",
        "        # do some division. or whatever\n",
        "        # add to dicts\n",
        "        pSpam[col] = spamCount / wordCount\n",
        "        pNot[col] = notCount / wordCount\n",
        "    # end dataset loop\n",
        "\n",
        "    # calculate overall probability that something is spam\n",
        "\n",
        "    totalSpam = 0\n",
        "    totalNot = 0\n",
        "\n",
        "    # loop over the rows, count how many are spam and how many aren't\n",
        "    # do some division or whatver\n",
        "    for row in training.iterrows():\n",
        "        if training[row, 57] == 1:\n",
        "            totalSpam = totalSpam + 1\n",
        "        else:\n",
        "            totalNot = totalNot + 1\n",
        "\n",
        "    prob = dict()\n",
        "\n",
        "    # for each email in test\n",
        "    for row in training.iterrows():\n",
        "        emailSpam = totalSpam\n",
        "        emailNot = totalNot\n",
        "\n",
        "        # for each word...\n",
        "        for col in range(54):\n",
        "            # if it has the word\n",
        "            if training[row, col] > 0:\n",
        "                # multiply probability that word = spam by probability of spam\n",
        "                emailSpam = emailSpam * pSpam[col]\n",
        "                # multiply probability that word != spam by probability of not spam\n",
        "                emailNot = emailNot * pNot[col]\n",
        "        # whichever probability is higher. say it is that.\n",
        "        if emailSpam > emailNot:\n",
        "            prob[col] = 1\n",
        "        else:\n",
        "            prob[col] = 0\n",
        "\n",
        "    return prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn(training, test):\n",
        "\n",
        "    distances = skl.metrics.pairwise_distances(training, test)\n",
        "\n",
        "    pred = dict()\n",
        "\n",
        "    for row in range(len(distances)): # that probably doesn't work but yknow\n",
        "        # fill array with the first val\n",
        "        close = list()\n",
        "        closest = distances[row][0]\n",
        "        # find closest rows...\n",
        "        for col in range(len(distances[row])):\n",
        "            if distances[row][col] <= closest:\n",
        "                closest = distances[row][col]\n",
        "                close.append(col)\n",
        "        # just get the 5 closest\n",
        "        # i am assuming that there are at least 5 that are in the list?\n",
        "        # there probably are. so it's fine.\n",
        "        close = close[-5:]\n",
        "\n",
        "        spam = 0\n",
        "        for i in range(5):\n",
        "            spam = spam + test[i, 57]\n",
        "        \n",
        "        if spam > 5 - spam:\n",
        "            pred[row] = 1\n",
        "        else:\n",
        "            pred[row] = 0\n",
        "    \n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lr(data):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "final actual real things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "(0, 57)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: (0, 57)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m val, train \u001b[38;5;241m=\u001b[39m loadData()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#naiveBayes(train, val)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mknn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[95], line 24\u001b[0m, in \u001b[0;36mknn\u001b[1;34m(training, test)\u001b[0m\n\u001b[0;32m     22\u001b[0m spam \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     spam \u001b[38;5;241m=\u001b[39m spam \u001b[38;5;241m+\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m57\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spam \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m-\u001b[39m spam:\n\u001b[0;32m     27\u001b[0m     pred[row] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: (0, 57)"
          ]
        }
      ],
      "source": [
        "val, train = loadData()\n",
        "naiveBayes(train, val)\n",
        "knn(train, val)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
