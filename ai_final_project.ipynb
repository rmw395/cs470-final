{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxgBX0YXu1du"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "1. load csv file (panda, numpy)\n",
        "2. split dataset. Example code:()\n",
        "   ```\n",
        "   random.shuffle(data) # change if you are using pandas dataframe\n",
        "   training = data[:int(len(data)*0.8)]\n",
        "   test = data[int(len(data)*0.8):]\n",
        "\n",
        "   fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "   for train_idx, val_idx in fold5.split(training):\n",
        "      sub_val = training[val_idx]\n",
        "      sub_train = training[train_idx]\n",
        "      clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "      performance(clf, test) # test the model on test dataset\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXyRfd35yRPd"
      },
      "source": [
        "# Naive bayes\n",
        "\n",
        "1. model learning:\n",
        "\n",
        "   Note:\n",
        "\n",
        "   features: remove attributes that is not related to word (the last four attributes)\n",
        "\n",
        "   labels: the last column\n",
        "\n",
        "   count P(c) -> how many samples are positive, and how many are negtive\n",
        "\n",
        "   if freq_word>0, then this word exists. You could use this to calculate P(a|c) -> for each class, what is the prob of each word\n",
        "\n",
        "   remember to use laplace smoothing.\n",
        "\n",
        "2. model evaluation (on val dataset -> performance(model, val)):\n",
        "   \n",
        "   for each new sample, $\\prod{P(a|c)}P(c)$ if word is in the email(freq_word > 0); and find the maximum class\n",
        "   \n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jRvHTlW0DYA"
      },
      "source": [
        "# KNN\n",
        "1. model learning: None\n",
        "\n",
        "2. model evaluation(on val dataset): You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "\n",
        "   ```\n",
        "   Note:\n",
        "   parallel programing\n",
        "   numpy.cos() to calcuate the similarity\n",
        "   ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUzUupva0Fxw"
      },
      "source": [
        "# LR\n",
        "\n",
        "1. model learning: You could use each row(exclude the last column) as the feature of the email. You do not have to recalcuate the freqency.\n",
        "    \n",
        "    $y = sigmoid(MX)$\n",
        "\n",
        "step 1: add one more column (all value is 1) in X -> X' = np.c_[np.ones((len(X), 1)), X]\n",
        "\n",
        "step 2:vector M = np.random.randn(len(X[0])+1, 1);\n",
        "\n",
        "key formula for step 3 (Note: n is the size of the TRAINING dataset; $cdot$ is dot production ):\n",
        "\n",
        "1. $pred_y = sigmoid(M\\cdot X')$\n",
        "\n",
        "2. $loss = -\\sum(y\\cdot log(pred_y)+(1-y)\\cdot log(1-pred_y))/n$\n",
        "\n",
        "3. $gm=X'\\cdot (pred_y - y)*2/n$\n",
        "\n",
        "Step 3 example code:\n",
        "   ```\n",
        "   #Step 3: performing gradient descent on whole dataset:\n",
        "   best_model = M\n",
        "   best_performace = 0\n",
        "   for i in range(epoch):\n",
        "     pred_y = ...\n",
        "     gm = ...\n",
        "     _p = performace(model, val)\n",
        "     if _p > best_performance:\n",
        "        best_model = M\n",
        "        best_performance = _p\n",
        "     M = M - learning_rate*gm\n",
        "   ```\n",
        "\n",
        "2. model evaluation(on val dataset):\n",
        "  \n",
        "   calculate pred_y, if more than 0.5, then the predicted label is 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAssSW_I0GvA"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0MQ0eo0MnmB"
      },
      "outputs": [],
      "source": [
        "def performance(model, data):\n",
        "  print(\"result:\")\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def loadData():\n",
        "    # load data\n",
        "    spam_data = pd.read_csv(\"spambase.csv\")\n",
        "\n",
        "    # shuffle data\n",
        "    spam_data = spam_data.sample(frac=1)\n",
        "\n",
        "    # first 80% is training, last 20% is test\n",
        "    training = spam_data[:int(len(spam_data)*0.8)]\n",
        "    test = spam_data[int(len(spam_data)*0.8):]\n",
        "\n",
        "    # use cross validation on training ?\n",
        "\n",
        "    '''\n",
        "    random.shuffle(data) # change if you are using pandas dataframe\n",
        "    training = data[:int(len(data)*0.8)]\n",
        "    test = data[int(len(data)*0.8):]\n",
        "\n",
        "    fold5 = KFold(5) # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
        "    for train_idx, val_idx in fold5.split(training):\n",
        "        sub_val = training[val_idx]\n",
        "        sub_train = training[train_idx]\n",
        "        clf = model(sub_train, sub_val, ...) # training the model, and evaluate it on validation dataset\n",
        "        performance(clf, test) # test the model on test dataset\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    naive bayes.\n",
        "\n",
        "    calculate probability of spam for each attribute (word)\n",
        "    compare to each email in test\n",
        "    see if the prediction is right\n",
        "    compare that to test dataset (take the higher probablity)\n",
        "\n",
        "    nvm. fr:\n",
        "\n",
        "    for each email in test...?\n",
        "    use the probabilities calculated from the training dataset\n",
        "    multiply the probabilities together?? like if it has a word in it,\n",
        "        use the probability (from training) that it is spam\n",
        "    multiply THAT by the general probability that smth is spam\n",
        "\n",
        "    then also do that for prob. not spam\n",
        "\n",
        "    whichever probability is  higher (spam/not spam). say that it is that.\n",
        "\n",
        "    keep track of well this training dataset did. (keep track of all of them actually.)\n",
        "    and then like. one of them did the best. yaaaa\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "knn\n",
        "\n",
        "pair-wise distance sklearn\n",
        "- will return a matrix with all the distances from everything to everything else\n",
        "\n",
        "go through all test emails\n",
        "- find closest traning emails to the current test email\n",
        "- pick the k closest neighbors, find the most frequent result\n",
        "- decide that it is that one."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
